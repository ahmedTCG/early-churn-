from pathlib import Path
import json
import warnings

import numpy as np
import pandas as pd
import joblib

from churn.features import build_customer_features

warnings.filterwarnings("ignore", category=RuntimeWarning)
warnings.filterwarnings("ignore", message=".*encountered in matmul.*")
np.seterr(over="ignore", divide="ignore", invalid="ignore")

ID_COL = "external_customerkey"
LOOKBACK_DAYS = 90

PROJECT_ROOT = Path(__file__).resolve().parents[1]
CLEANED_FILE = PROJECT_ROOT / "cleaned_events.parquet"
ACTIVITY_FILE = PROJECT_ROOT / "activity_events.parquet"

ART_DIR = PROJECT_ROOT / "artifacts"
MODEL_FILE = ART_DIR / "churn_model.joblib"
FEATURES_FILE = ART_DIR / "feature_list.json"

OUT_PARQUET = PROJECT_ROOT / "customer_scores.parquet"
OUT_CSV = PROJECT_ROOT / "customer_scores.csv"
OUT_BUCKETED_CSV = PROJECT_ROOT / "customer_scores_bucketed.csv"
OUT_BUCKET_THRESHOLDS = ART_DIR / "bucket_thresholds.json"


def get_max_time() -> pd.Timestamp:
    s = pd.to_datetime(
        pd.read_parquet(ACTIVITY_FILE, columns=["event_time"])["event_time"],
        errors="coerce",
        utc=True,
    )
    return s.max()


def load_events(window_start: pd.Timestamp, window_end: pd.Timestamp) -> pd.DataFrame:
    cols = [ID_COL, "event_time", "interaction_type"]
    df = pd.read_parquet(ACTIVITY_FILE, columns=cols)

    df["event_time"] = pd.to_datetime(df["event_time"], errors="coerce", utc=True)
    df = df.dropna(subset=[ID_COL, "event_time", "interaction_type"]).copy()
    df = df[(df["event_time"] > window_start) & (df["event_time"] <= window_end)].copy()

    df[ID_COL] = df[ID_COL].astype("string").str.strip().astype("category")
    df["interaction_type"] = df["interaction_type"].astype("string").str.strip().str.lower().astype("category")
    return df


def get_weak_signal_customers(activity_customers: set, snapshot_time: pd.Timestamp) -> pd.DataFrame:
    """
    Find customers who exist in cleaned_events but have NO activity events.
    These are customers with only bounce/cancel/unsub events.
    """
    if not CLEANED_FILE.exists():
        return pd.DataFrame(columns=[ID_COL])
    
    # Get all customers from cleaned data
    cleaned = pd.read_parquet(CLEANED_FILE, columns=[ID_COL])
    cleaned[ID_COL] = cleaned[ID_COL].astype("string").str.strip()
    all_customers = set(cleaned[ID_COL].unique())
    
    # Find customers NOT in activity events
    weak_signal_ids = all_customers - activity_customers
    
    if not weak_signal_ids:
        return pd.DataFrame(columns=[ID_COL])
    
    # Create DataFrame for weak signal customers
    weak_df = pd.DataFrame({
        ID_COL: list(weak_signal_ids),
        "snapshot_time": snapshot_time,
        "churn_probability": 1.0,
        "churn_bucket": "weak_signal",
        "churn_action": 1,  # Flag for action (they need attention)
    })
    
    return weak_df


def make_X(feats: pd.DataFrame, feature_list: list) -> pd.DataFrame:
    X = feats.drop(columns=[ID_COL], errors="ignore")
    X = X.select_dtypes(include=[np.number]).replace([np.inf, -np.inf], np.nan).fillna(0)

    if "recency_days" in X.columns:
        X["recency_days"] = X["recency_days"].clip(upper=365)

    for c in list(X.columns):
        if c.startswith(("n_events_last_", "active_days_last_", "cnt_", "n_session_events_", "n_email_engagement_", "n_orders_")):
            X[c] = np.log1p(X[c])

    for c in feature_list:
        if c not in X.columns:
            X[c] = 0

    return X[feature_list]


def main():
    if not ACTIVITY_FILE.exists():
        raise FileNotFoundError("Missing activity_events.parquet. Run: python scripts/activity.py")

    if not MODEL_FILE.exists() or not FEATURES_FILE.exists():
        raise FileNotFoundError("Missing model artifacts. Run: python scripts/train.py")

    model = joblib.load(MODEL_FILE)
    feature_list = json.loads(FEATURES_FILE.read_text(encoding="utf-8"))

    snapshot_time = get_max_time()
    window_start = snapshot_time - pd.Timedelta(days=LOOKBACK_DAYS)

    # Load activity events and build features
    events = load_events(window_start, snapshot_time)
    activity_customers = set(events[ID_COL].unique())

    feats = build_customer_features(events, snapshot_time=snapshot_time)
    X = make_X(feats, feature_list)

    proba = model.predict_proba(X)[:, 1]

    # Score customers with activity events
    scored = feats[[ID_COL]].copy()
    scored["snapshot_time"] = snapshot_time
    scored["churn_probability"] = proba

    # --- Bucketization (percentile-based) ---
    p90 = scored["churn_probability"].quantile(0.90)
    p70 = scored["churn_probability"].quantile(0.70)
    p40 = scored["churn_probability"].quantile(0.40)

    def bucketize(p: float) -> str:
        if p >= p90:
            return "very_high"
        elif p >= p70:
            return "high"
        elif p >= p40:
            return "medium"
        else:
            return "low"

    scored["churn_bucket"] = scored["churn_probability"].apply(bucketize)
    scored["churn_action"] = scored["churn_bucket"].isin(["high", "very_high"]).astype(int)

    # --- Add weak signal customers ---
    weak_signal_df = get_weak_signal_customers(activity_customers, snapshot_time)
    n_weak_signal = len(weak_signal_df)

    # Combine scored customers with weak signal customers
    if n_weak_signal > 0:
        out = pd.concat([scored, weak_signal_df], ignore_index=True)
    else:
        out = scored

    # --- Save bucket thresholds ---
    ART_DIR.mkdir(exist_ok=True)
    OUT_BUCKET_THRESHOLDS.write_text(
        json.dumps({
            "snapshot_time": str(snapshot_time),
            "p40_medium_min": float(p40),
            "p70_high_min": float(p70),
            "p90_very_high_min": float(p90),
            "action_rule": "high_or_very_high_or_weak_signal",
            "weak_signal_note": "Customers with no activity events (only bounce/cancel/unsub)",
        }, indent=2),
        encoding="utf-8"
    )

    # --- Save outputs ---
    out.to_parquet(OUT_PARQUET, index=False)
    out.to_csv(OUT_CSV, index=False)
    out.to_csv(OUT_BUCKETED_CSV, index=False)

    # --- Print summary ---
    print("=" * 60)
    print("SCORING RESULTS")
    print("=" * 60)
    print(f"Snapshot time: {snapshot_time}")
    print()
    
    print("Customers scored by model:")
    print(f"  Total: {len(scored):,}")
    print()
    
    print("Bucket distribution (model-scored):")
    bucket_counts = scored["churn_bucket"].value_counts()
    bucket_pct = scored["churn_bucket"].value_counts(normalize=True)
    for bucket in ["very_high", "high", "medium", "low"]:
        if bucket in bucket_counts.index:
            print(f"  {bucket:10}: {bucket_counts[bucket]:>7,} ({bucket_pct[bucket]:.1%})")
    
    print()
    print("Bucket thresholds:")
    print(f"  very_high >= {p90:.3f}")
    print(f"  high      >= {p70:.3f}")
    print(f"  medium    >= {p40:.3f}")
    print(f"  low       <  {p40:.3f}")
    
    print()
    print("Weak signal customers (no activity events):")
    print(f"  Total: {n_weak_signal:,}")
    print(f"  These customers have only bounce/cancel/unsub events")
    print(f"  Assigned: churn_probability=1.0, churn_bucket='weak_signal'")
    
    print()
    print("Combined output:")
    print(f"  Total customers: {len(out):,}")
    print(f"  - Model scored:  {len(scored):,}")
    print(f"  - Weak signal:   {n_weak_signal:,}")
    
    print()
    print("Overall bucket distribution:")
    print(out["churn_bucket"].value_counts().sort_index())
    
    print()
    print(f"Saved parquet: {OUT_PARQUET}")
    print(f"Saved csv: {OUT_CSV}")
    print(f"Saved bucket thresholds: {OUT_BUCKET_THRESHOLDS}")


if __name__ == "__main__":
    main()